{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6881e307-f8ff-48b0-aab8-2a1c174bd7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "text_file = tf.keras.utils.get_file(\n",
    "    fname = 'fra-eng.zip',\n",
    "    origin = \"http://storage.googleapis.com/download.tensorflow.org/data/fra-eng.zip\",\n",
    "    extract = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a706f1fd-a067-4c27-9023-6ee04954048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib \n",
    "\n",
    "text_file = pathlib.Path(text_file).parent / 'fra.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6d6ce415-4a85-4372-a6cd-a4a9fea2c9c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\.keras\\datasets\\fra.txt\n"
     ]
    }
   ],
   "source": [
    "print(text_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fefbcae4-043e-450b-a710-b4e7b0ea59e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in extracted folder: ['fra.txt', '_about.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Path to the extracted folder\n",
    "extracted_path = os.path.join(os.path.expanduser('~'), '.keras', 'datasets', 'fra-eng_extracted')\n",
    "\n",
    "# List files inside that folder\n",
    "print(\"Files in extracted folder:\", os.listdir(extracted_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "22838c9a-5c68-4ed8-b8ec-fe6d87da4fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: Go.\tVa !\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.join(extracted_path, 'fra.txt')\n",
    "\n",
    "with open(file_path, encoding='utf-8') as fp:\n",
    "    text_pairs = [line.strip() for line in fp]\n",
    "\n",
    "# Optional: show a sample\n",
    "print(\"Example:\", text_pairs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e537e26d-f8a0-41ae-99e6-466a18bda7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: ['Go.', 'Hi.', 'Run!', 'Run!', 'Who?']\n",
      "French: ['Va !', 'Salut !', 'Cours\\u202f!', 'Courez\\u202f!', 'Qui ?']\n"
     ]
    }
   ],
   "source": [
    "eng_sentences = []\n",
    "fra_sentences = []\n",
    "\n",
    "for line in text_pairs:\n",
    "    eng, fra = line.split('\\t')\n",
    "    eng_sentences.append(eng)\n",
    "    fra_sentences.append(fra)\n",
    "\n",
    "print(\"English:\", eng_sentences[:5])\n",
    "print(\"French:\", fra_sentences[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a83b0fc6-b60a-4494-b0c1-f2a16c3e9cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def normalize(text):\n",
    "    text = unicodedata.normalize(\"NFD\", text)\n",
    "    text = text.encode(\"ascii\", \"ignore\").decode(\"utf-8\")  # remove accents\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2be31e10-c9ab-46d5-b5ea-900d4c311695",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, encoding='utf-8') as fp:\n",
    "    text_pairs = [normalize(line) for line in fp]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4460ac0b-b9db-4967-80c2-ad01fbc10a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English: Asia is much larger than Australia.\n",
      "French:  L'Asie est beaucoup plus grande que l'Australie.\n",
      "------------------------------\n",
      "English: I thought you went home.\n",
      "French:  Je pensais que vous étiez allé chez vous.\n",
      "------------------------------\n",
      "English: She suddenly kissed me.\n",
      "French:  Elle m'embrassa subitement.\n",
      "------------------------------\n",
      "English: Do you know why Tom doesn't like Mary?\n",
      "French:  Sais-tu pourquoi Tom n'aime pas Mary ?\n",
      "------------------------------\n",
      "English: You're not safe here.\n",
      "French:  Vous n'êtes pas en sécurité, ici.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Assuming eng_sentences and fra_sentences are already created\n",
    "test_pair = list(zip(eng_sentences, fra_sentences))\n",
    "\n",
    "for _ in range(5):\n",
    "    eng, fra = random.choice(test_pair)\n",
    "    print(f\"English: {eng}\")\n",
    "    print(f\"French:  {fra}\")\n",
    "    print('-' * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1eb81bdf-1a5a-49e6-879b-d3541d88aef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens in English: 0\n",
      "Total tokens in French: 0\n",
      "Maximum English sentence length: 0\n",
      "Maximum French sentence length: 0\n"
     ]
    }
   ],
   "source": [
    "eng_tokens, fre_tokens = set(), set()\n",
    "eng_maxlen, fre_maxlen = 0, 0\n",
    "\n",
    "for line in text_pairs:\n",
    "    parts = line.split('\\t')\n",
    "    if len(parts) != 2:\n",
    "        continue  # skip malformed lines\n",
    "\n",
    "    eng, fre = parts\n",
    "    eng_words = eng.split()\n",
    "    fre_words = fre.split()\n",
    "\n",
    "    # Add tokens to vocab sets\n",
    "    eng_tokens.update(eng_words)\n",
    "    fre_tokens.update(fre_words)\n",
    "\n",
    "    # Update max lengths\n",
    "    eng_maxlen = max(eng_maxlen, len(eng_words))\n",
    "    fre_maxlen = max(fre_maxlen, len(fre_words))\n",
    "\n",
    "# ✅ Final output\n",
    "print(f\"Total tokens in English: {len(eng_tokens)}\")\n",
    "print(f\"Total tokens in French: {len(fre_tokens)}\")\n",
    "print(f\"Maximum English sentence length: {eng_maxlen}\")\n",
    "print(f\"Maximum French sentence length: {fre_maxlen}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1e26281b-3f0c-4e8a-9f75-6721170a614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"text-pair.pickel\" , 'wb') as fp:\n",
    "    pickle.dump(text_pairs, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a89648d-cbca-4977-838a-7b5229a73aea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
